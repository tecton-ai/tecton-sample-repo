{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unstructured.partition.md import partition_md\n",
    "from unstructured.chunking.dispatch import chunk\n",
    "from tecton_gen_ai.api import Agent\n",
    "from multiprocess import Pool\n",
    "import pandas as pd\n",
    "import os\n",
    "import tqdm\n",
    "from pydantic import BaseModel, Field\n",
    "from tecton_gen_ai.testing import set_dev_mode\n",
    "\n",
    "set_dev_mode()\n",
    "\n",
    "class Declarations(BaseModel):\n",
    "    declarations: list[list[str,str]] = Field(..., description=\"\"\"List of tuples of declarations.\n",
    "Each tuple contains the object/function name and the description.\n",
    "\n",
    "You should only extract:\n",
    "- Tecton classes and docorated functions\n",
    "- Tecton objects embedded in other objects (e.g. SnowflakeConfig in BatchSource, Attribute and Aggregate)\n",
    "- Unit tests (set the first value in the tuple as \"test\")\n",
    "                                              \n",
    "Pay attention to the import statements at the beginning that tells you which objects and functions are imported from Tecton.\n",
    "\n",
    "Don't extract declarations that are commented out\n",
    "\n",
    "The description should be under 100 words\n",
    "\n",
    "\n",
    "For example, with this code:\n",
    "\n",
    "```from tecton import Entity, FeatureTable, Attribute\n",
    "from tecton.types import String, Timestamp, Int64, Field\n",
    "from fraud.entities import user\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "features = [\n",
    "    Attribute('user_login_count_7d', Int64),\n",
    "    Attribute('user_login_count_30d', Int64),\n",
    "]\n",
    "\n",
    "user_login_counts = FeatureTable(\n",
    "    name='user_login_counts',\n",
    "    entities=[user],\n",
    "    features=features,\n",
    "    online=True,\n",
    "    offline=True,\n",
    "    ttl=timedelta(days=7),\n",
    "    owner='demo-user@tecton.ai',\n",
    "    tags={'release': 'production'},\n",
    "    description='User login counts over time.',\n",
    "    timestamp_field='timestamp'\n",
    ")\n",
    "```\n",
    "\n",
    "The declarations would be:\n",
    "\n",
    "[(\"FeatureTable\", \"User login counts over time.\")]\n",
    "\n",
    "In this code\n",
    "\n",
    "```python\n",
    "fraud_detection_feature_service = FeatureService(\n",
    "    name='fraud_detection_feature_service',\n",
    "    prevent_destroy=False,  # Set to True for production services to prevent accidental destructive changes or downtime.\n",
    "    features=[\n",
    "        transaction_amount_is_higher_than_average,\n",
    "        user_transaction_amount_metrics,\n",
    "        user_transaction_counts,\n",
    "        user_distinct_merchant_transaction_count_30d,\n",
    "        merchant_fraud_rate\n",
    "    ]\n",
    ")\n",
    "\n",
    "minimal_fs = FeatureService(\n",
    "     name='minimal_fs',\n",
    "     features=[\n",
    "         transaction_amount_is_high\n",
    "     ]\n",
    ")\n",
    "```\n",
    "\n",
    "The declarations would be:\n",
    "\n",
    "[\n",
    "    (\"FeatureService\", \"Fraud detection feature service\"),\n",
    "    (\"FeatureService\", \"Whether transaction amount is higher\")\n",
    "]\n",
    "\n",
    "In this code:\n",
    "\n",
    "```\n",
    "import math\n",
    "\n",
    "from ads.features.on_demand_feature_views.user_query_embedding_similarity import user_query_embedding_similarity\n",
    "\n",
    "\n",
    "# Testing the 'user_query_embedding_similarity' feature which takes in request data ('query_embedding')\n",
    "# and a precomputed feature ('user_embedding') as inputs\n",
    "def test_user_query_embedding_similarity():\n",
    "    request = {'query_embedding': [1.0, 1.0, 0.0]}\n",
    "    user_embedding = {'user_embedding': [0.0, 1.0, 1.0]}\n",
    "\n",
    "    actual = user_query_embedding_similarity.test_run(request=request, user_embedding=user_embedding)\n",
    "\n",
    "    # Float comparison.\n",
    "    expected = 0.5\n",
    "    assert math.isclose(actual['cosine_similarity'], expected)\n",
    "```\n",
    "\n",
    "The declarations would be:\n",
    "\n",
    "[(\"test\", \"Testing the 'user_query_embedding_similarity' feature which takes in request data ('query_embedding') and a precomputed feature ('user_embedding') as inputs\")]\n",
    "\n",
    "In this code\n",
    "                                              \n",
    "```python\n",
    "from tecton import BatchSource, SnowflakeConfig\n",
    "from tecton.types import Field, Int64, String, Timestamp, Array\n",
    "\n",
    "gaming_user_batch = BatchSource(\n",
    "    name=\"gaming_users\",\n",
    "    batch_config=SnowflakeConfig(\n",
    "      database=\"VINCE_DEMO_DB\",\n",
    "      schema=\"PUBLIC\",\n",
    "      table=\"ONLINE_GAMING_USERS\",\n",
    "      url=\"https://<your-cluster>.<your-snowflake-region>.snowflakecomputing.com/\",\n",
    "      warehouse=\"COMPUTE_WH\",\n",
    "      timestamp_field='TIMESTAMP',\n",
    "    ),\n",
    ")\n",
    "```\n",
    "\n",
    "(Pay attention that SnowflakeConfig is a configuration object embedded in the BatchSource object, we also need to extract that)\n",
    "\n",
    "The declarations would be:\n",
    "\n",
    "[(\"BatchSource\", \"Gaming users batch source\"), (\"SnowflakeConfig\", \"Gaming users batch source configuration\")]                                           \n",
    "\"\"\")\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"code_parser\",\n",
    "    prompt=\"Extract the function, class or test declarations from the code.\",\n",
    "    output_schema=Declarations,\n",
    "    llm = {\n",
    "        \"model\": \"openai/gpt-4o-2024-11-20\",\n",
    "        \"temperature\": 0,\n",
    "        \"timeout\": 30,\n",
    "        \"max_tokens\": 2000,\n",
    "    }\n",
    ")\n",
    "\n",
    "def get_py_files(directory):\n",
    "    files = []\n",
    "    for root, dirs, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.py'):\n",
    "                files.append(os.path.join(root, filename))\n",
    "    return files\n",
    "\n",
    "def extract_declarations(folders):\n",
    "    files = []\n",
    "    for folder in folders:\n",
    "        files+= get_py_files(folder)\n",
    "    res = []\n",
    "    for i in tqdm.tqdm(range(len(files))):\n",
    "        with open(files[i], 'r') as f:\n",
    "            code = f.read()\n",
    "        for d in agent.invoke(code)[\"declarations\"]:\n",
    "            res.append({\"text\": f\"Example of {d[0]}. {d[1]}\", \"code\": code}) \n",
    "    return res\n",
    "\n",
    "\n",
    "def partition(path, **kwargs):\n",
    "    with open(path, 'r') as f:\n",
    "        markdown_text = f.read()\n",
    "    replaced_text, code_snippets = extract_and_replace(markdown_text)\n",
    "    replacer = CodeReplacer(code_snippets)\n",
    "    elements = partition_md(text=replaced_text, languages=[\"eng\"], **kwargs)\n",
    "    for element in elements:\n",
    "        element.apply(replacer.apply)\n",
    "        yield element\n",
    "\n",
    "def chunk_md(path, chunking_strategy, max_characters, **kwargs):\n",
    "    elements = partition(path, **kwargs)\n",
    "    chunks = chunk(elements, chunking_strategy=chunking_strategy, max_characters=max_characters)\n",
    "    for ck in chunks:\n",
    "        yield {'text': ck.text, \"id\": ck.id}\n",
    "\n",
    "def clean_source_id(source: str) -> str:\n",
    "    if source.endswith(\".md\"):\n",
    "        src = source[:-3]\n",
    "    else:\n",
    "        src = source\n",
    "    if src.endswith(\"/changelog\"):\n",
    "        return \"changelog\"\n",
    "    parts = src.split(\"/\")\n",
    "    # there can be duplications in the last n parts, dedup\n",
    "    while(len(parts) > 1 and parts[-1] == parts[-2]):\n",
    "        parts = parts[:-1]\n",
    "    return \"/\".join(parts)\n",
    "\n",
    "def generate_docs(version, base_path, chunking_strategy, max_characters, concurrency, url_func, **kwargs):\n",
    "    files = get_md_files(base_path)\n",
    "    files += [\"../../changelog.md\"]\n",
    "\n",
    "    def process_file(file):\n",
    "        source_id = clean_source_id(os.path.relpath(file, base_path))\n",
    "        if any(x.startswith(\"_\") for x in source_id.split(\"/\")):\n",
    "            return None\n",
    "        with open(file, 'r') as f:\n",
    "            markdown_text = f.read()\n",
    "        df = pd.DataFrame(chunk_md(file, chunking_strategy=chunking_strategy, max_characters=max_characters, **kwargs))        \n",
    "        df[\"source\"] = source_id\n",
    "        return pd.DataFrame([{\"source\":source_id, \"version\":version, \"url\": url_func(source_id), \"text\":markdown_text}]), df\n",
    "\n",
    "    #with Pool(concurrency) as pool:\n",
    "    #    raw = pool.map(process_file, files)\n",
    "    raw = []\n",
    "    for i in tqdm.tqdm(range(len(files))):\n",
    "        res = process_file(files[i])\n",
    "        if res is not None:\n",
    "            raw.append(process_file(files[i]))\n",
    "    # Flatten the list of lists\n",
    "    chunks = pd.concat([sublist for _, sublist in raw])\n",
    "    texts = pd.concat([md for md, _ in raw])\n",
    "    return texts, chunks\n",
    "\n",
    "def to_url(source_id, prefix) -> str:\n",
    "    if source_id == \"changelog\":\n",
    "        return \"https://docs.tecton.ai/changelog\"\n",
    "    return prefix + source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162/162 [03:22<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "res = extract_declarations([\"../../rift\", \"../../spark\", \"../../../examples\"])\n",
    "df=pd.DataFrame(res)\n",
    "df.to_parquet(\"/tmp/examples.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Example of Entity. An ad</td>\n",
       "      <td>from tecton import Entity\\nfrom tecton.types i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Example of Entity. Content ID</td>\n",
       "      <td>from tecton import Entity\\nfrom tecton.types i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Example of Entity. Auction ID</td>\n",
       "      <td>from tecton import Entity\\nfrom tecton.types i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Example of Entity. A user of the platform</td>\n",
       "      <td>from tecton import Entity\\nfrom tecton.types i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Example of Entity. The keyword describing the ...</td>\n",
       "      <td>from tecton import Entity\\nfrom tecton.types i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>Example of Attribute. Number of ratings for ca...</td>\n",
       "      <td>from tecton import realtime_feature_view\\nfrom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Example of stream_feature_view. Ratings summar...</td>\n",
       "      <td>from Recommender_system.data_sources import ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Example of Aggregate. Aggregate function for t...</td>\n",
       "      <td>from Recommender_system.data_sources import ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Example of batch_feature_view. Book aggregate ...</td>\n",
       "      <td>from Recommender_system.entities import book\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Example of Aggregate. Aggregate function for b...</td>\n",
       "      <td>from Recommender_system.entities import book\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text                                               code\n",
       "0                             Example of Entity. An ad  from tecton import Entity\\nfrom tecton.types i...\n",
       "1                        Example of Entity. Content ID  from tecton import Entity\\nfrom tecton.types i...\n",
       "2                        Example of Entity. Auction ID  from tecton import Entity\\nfrom tecton.types i...\n",
       "3            Example of Entity. A user of the platform  from tecton import Entity\\nfrom tecton.types i...\n",
       "4    Example of Entity. The keyword describing the ...  from tecton import Entity\\nfrom tecton.types i...\n",
       "..                                                 ...                                                ...\n",
       "243  Example of Attribute. Number of ratings for ca...  from tecton import realtime_feature_view\\nfrom...\n",
       "244  Example of stream_feature_view. Ratings summar...  from Recommender_system.data_sources import ra...\n",
       "245  Example of Aggregate. Aggregate function for t...  from Recommender_system.data_sources import ra...\n",
       "246  Example of batch_feature_view. Book aggregate ...  from Recommender_system.entities import book\\n...\n",
       "247  Example of Aggregate. Aggregate function for b...  from Recommender_system.entities import book\\n...\n",
       "\n",
       "[248 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
